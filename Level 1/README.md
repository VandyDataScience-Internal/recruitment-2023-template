# ðŸŒ´ Welcome to Level 1 of the 2023 EM/Dev Assessment! ðŸŒ´
This is the first level of the dev assignment and the beginning level to start at. In this level,
you will be finding and cleaning a dataset so that it can be used for later analysis.
<br> 
remember that you will be using this dataset in further levels to solve research 
questions and build data models, so data cleaned in this level should be fully usable
by the next level. Chose data that interests you, and get creative!


## Objective:
Your task will be to **find** and **clean** a dataset. This means choosing a dataset from 
online sources, and then filtering the entries to be usable for modelling. 
Then, you must a readme file that cites the dataset used and explains why you chose it.
Cleaning a dataset refers to removing any data entries that are considered 'bad' or unusable for modelling. Unusable data can be
entries with incorrect answers, blank and incomplete entries, 
statistical anomalies, etc.. These are found and filtered out of the dataset when cleaning.
<br>
This objective is intended to be an entry point to thinking about what type of data you want
to research, and how to use programming methods in order to filter that data. This is 
meant to be a simple primer for data analysis, so don't overthink it!


## Requirements:
- One complete dataset.
- A file that fully cleans said dataset using programming methods. Any language can be used, but python 
  is highly encouraged!
- A readme file that includes:
  - Your full name
  - The source for your dataset
  - A short explanation on why you chose your dataset. Does it interest you? What makes it high-quality? 
- Comments on, at minimum, every major program component.


## How do I find a data set?
If you don't know where to start, there are plenty of online databases for finding
datasets. [Kaggle](https://www.kaggle.com/) is a great source of curated databases, and 
[data.gov](data.gov) has databases from the US government. Search engines are also encouraged
to find datasets. We allow for a lot of flexibility on where you can get a data: no sources are completely
off-limits, however part of our grading will be based on the quality and validity of your
source. Think about stuff like the data's size, documentation, formatting, and the
credibility of it's source.


## How do I clean a data set?
Unless you want to agonize through hundreds of hours scrolling down an Excel spreadsheet, data is cleaned
using programs specifically made to filter through keywords in formatted datasets.
[this article](https://www.tableau.com/learn/articles/what-is-data-cleaning)
explains what cleaning data is and steps that can be taken, and
[this article](https://towardsdatascience.com/so-youve-got-a-dataset-here-s-how-you-clean-it-5d0b04a2ed86)
has many specific cleaning programs that can be done in python. Google is your friend! 
How data is cleaned depends on the format and type of the dataset, but python is very extensive and intuitive in 
data management, making it ideal for data science.


## You will be graded on:
- The quality of the dataset chosen and your reasoning behind choosing it.
- How thoroughly the dataset has been cleaned, with how many steps are made to ensure the
  data is usable
- The readability of your code. Good programs will be simple to understand, modular, and 
  have proper documentaiton so that someone other than the programmer can easily understand your
  code.
- The quality of your code, with proper and effective methods used for dataset cleaning.
- Following proper and thorough commenting etiquette.



